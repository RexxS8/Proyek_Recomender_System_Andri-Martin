# -*- coding: utf-8 -*-
"""Recomendation-Movie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TeYeUq9ahOtbQzg4CQtPgNsrViq2Tfcj

# **1. Perkenalan Dataset**

Collaborative Filtering (Item-to-Item Based) untuk sistem rekomendasi film, Dataset terdiri dari dua file utama:
- movies.csv: berisi informasi judul film dan movieId
- ratings.csv: berisi rating yang diberikan user terhadap film tertentu

Tujuan: Membangun sistem rekomendasi berdasarkan kemiripan film dari pola rating pengguna lain

# **2. Import Library**

Pada tahap ini, Anda perlu mengimpor beberapa pustaka (library) Python yang dibutuhkan untuk analisis data dan pembangunan model machine learning.
"""

import numpy as np
import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Dropout, Concatenate, Add
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import MeanAbsoluteError

"""# **3. Memuat Dataset**

Pada tahap ini, Anda perlu memuat dataset ke dalam notebook. Jika dataset dalam format CSV, Anda bisa menggunakan pustaka pandas untuk membacanya. Pastikan untuk mengecek beberapa baris awal dataset untuk memahami strukturnya dan memastikan data telah dimuat dengan benar.

Jika dataset berada di Google Drive, pastikan Anda menghubungkan Google Drive ke Colab terlebih dahulu. Setelah dataset berhasil dimuat, langkah berikutnya adalah memeriksa kesesuaian data dan siap untuk dianalisis lebih lanjut.
"""

# Load dataset dari CSV
movies = pd.read_csv('data/movies.csv')
ratings = pd.read_csv('data/ratings.csv')

# Menampilkan beberapa baris awal
print("Movies dataset:")
movies.head()

print("Ratings dataset:")
ratings.head()

movies.shape

ratings.shape

"""# **4. Exploratory Data Analysis (EDA)**

Pada tahap ini, Anda akan melakukan **Exploratory Data Analysis (EDA)** untuk memahami karakteristik dataset.
Tujuan dari EDA adalah untuk memperoleh wawasan awal yang mendalam mengenai data dan menentukan langkah selanjutnya dalam analisis atau pemodelan.
"""

# Bentuk pivot tabel dari rating
data = pd.pivot_table(ratings, index='movieId', columns='userId', values='rating')

# Distribusi jumlah vote per movie dan per user
numberOf_user_voted_for_movie = ratings.groupby('movieId')['rating'].agg('count').reset_index()
numberOf_movies_voted_by_user = ratings.groupby('userId')['rating'].agg('count').reset_index()

# Visualisasi threshold filter
plt.figure()
sns.scatterplot(y='rating', x='movieId', data=numberOf_user_voted_for_movie)
plt.axhline(y=10, color='r')
plt.ylabel('Number Of Users Voted for Movie')
plt.title('Jumlah rating per movie')

plt.figure()
sns.scatterplot(y='rating', x='userId', data=numberOf_movies_voted_by_user)
plt.axhline(y=60, color='r')
plt.ylabel('Number Of Movies rated by user')
plt.title('Jumlah rating per user')
plt.show()

plt.figure()
sns.histplot(ratings['rating'], bins=10, kde=True)
plt.title("Distribusi Rating")
plt.show()

top_movies = ratings.groupby('movieId')['rating'].agg(['mean', 'count'])
top_movies = top_movies.merge(movies, on='movieId')
top_movies.sort_values('count', ascending=False).head(10)

plt.figure(figsize=(8, 6))
sns.boxplot(x=ratings['rating'])
plt.title('Boxplot Distribusi Rating Film')
plt.xlabel('Rating')
plt.show()

# 2. Hitung jumlah rating per film dan rata-rata rating
movie_stats = ratings.groupby('movieId').agg(
    rating_count=('rating', 'count'),
    rating_mean=('rating', 'mean')
).reset_index()

# Filter film dengan minimal 100 rating
popular_movies = movie_stats[movie_stats['rating_count'] > 100]

# Gabungkan dengan data judul film
popular_movies = popular_movies.merge(movies[['movieId', 'title']], on='movieId')

# Ambil top 10 film berdasarkan rating rata-rata tertinggi
top10_movies = popular_movies.sort_values(by='rating_mean', ascending=False).head(10)

print("Top 10 Film dengan Rating Tertinggi (dengan minimal 100 rating):")
print(top10_movies[['title', 'rating_mean', 'rating_count']])

# Visualisasi top 10 film dengan barplot
plt.figure(figsize=(12, 6))
sns.barplot(data=top10_movies, x='rating_mean', y='title', palette='viridis')
plt.xlabel('Rata-rata Rating')
plt.ylabel('Judul Film')
plt.title('Top 10 Film dengan Rating Tertinggi (vote > 100)')
plt.xlim(0, 5)  # Rentang rating 0-5
plt.show()

"""# **5. Data Preprocessing**

Pada tahap ini, data preprocessing adalah langkah penting untuk memastikan kualitas data sebelum digunakan dalam model machine learning. Data mentah sering kali mengandung nilai kosong, duplikasi, atau rentang nilai yang tidak konsisten, yang dapat memengaruhi kinerja model. Oleh karena itu, proses ini bertujuan untuk membersihkan dan mempersiapkan data agar analisis berjalan optimal.
"""

# 5. Data Preprocessing
data.fillna(0, inplace=True)
data_final = data.loc[numberOf_user_voted_for_movie[numberOf_user_voted_for_movie['rating'] > 10]['movieId'], :]
data_final = data_final.loc[:, numberOf_movies_voted_by_user[numberOf_movies_voted_by_user['rating'] > 60]['userId']]
data_final.reset_index(inplace=True)
csr_data = csr_matrix(data_final.drop('movieId', axis=1).values)

"""# **6. Pembangunan Model Collaborative Filtering (KNN)**

Pada tahap ini, dilakukan pembangunan sistem rekomendasi menggunakan pendekatan **Collaborative Filtering berbasis item** dengan algoritma **K-Nearest Neighbors (KNN)**. Metode ini mencari kesamaan antar item (film) berdasarkan pola rating dari pengguna.

### Tahapan Model

1. **Representasi Data**  
   Dataset diubah menjadi **matriks sparse (CSR â€“ Compressed Sparse Row)**, di mana baris merepresentasikan film dan kolom merepresentasikan pengguna, dengan nilai berupa rating.

2. **Pemodelan KNN**  
   Model KNN dilatih menggunakan **metrik cosine similarity** untuk mengukur kedekatan antar film berdasarkan pola rating pengguna.

3. **Fungsi Rekomendasi**
   - Menerima input berupa **nama film**.
   - Mencari **ID film** yang cocok.
   - Mengambil film-film yang paling mirip berdasarkan hasil dari `kneighbors`.
   - Mengembalikan daftar **rekomendasi film beserta tingkat kemiripannya** (cosine distance).

### Tujuan

Memberikan **rekomendasi film serupa** berdasarkan pola rating pengguna lain terhadap film tersebut. Pendekatan ini efektif ketika terdapat cukup data interaksi pengguna dan item, **meskipun tidak memperhitungkan informasi tambahan** seperti genre, sinopsis, atau profil pengguna.
"""

# 6. Pembangunan Model Collaborative Filtering (KNN)
# Buat TF-IDF matrix untuk judul film (sekali saja, di luar fungsi)
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['title'])

# Buat model KNN dengan metric cosine similarity dan algoritma brute force
knn = NearestNeighbors(metric='cosine', algorithm='brute')
knn.fit(csr_data)

# Precompute rata-rata rating tiap film untuk lookup cepat
movie_rating_mean = ratings.groupby('movieId')['rating'].mean()

def get_movie_recommendation(movie_name):
    n = 10

    # 1. Cari film dengan str.contains dulu
    movie_list = movies[movies['title'].str.contains(movie_name, case=False)]

    # 2. Jika tidak ada hasil, pakai TF-IDF + cosine similarity untuk cari judul mirip
    if len(movie_list) == 0:
        # Vectorize input movie_name
        movie_name_tfidf = tfidf.transform([movie_name])
        # Hitung cosine similarity dengan semua judul film
        cosine_sim = cosine_similarity(movie_name_tfidf, tfidf_matrix).flatten()
        # Ambil indeks film dengan similarity tertinggi > threshold (misal 0.3)
        max_sim_idx = np.argmax(cosine_sim)
        if cosine_sim[max_sim_idx] < 0.3:
            return "Tidak ditemukan judul film yang cocok. Silakan cek kembali input."
        movie_list = movies.iloc[[max_sim_idx]]

    movie_id = movie_list.iloc[0]['movieId']
    if movie_id not in data_final['movieId'].values:
        return "Film tersebut tidak ada dalam model rekomendasi."

    movie_idx = data_final[data_final['movieId'] == movie_id].index[0]
    distances, indices = knn.kneighbors(csr_data[movie_idx], n_neighbors=n+1)
    rec_movie_indices = sorted(list(zip(indices.squeeze(), distances.squeeze())), key=lambda x: x[1])[1:]

    recommendations = []
    for val in rec_movie_indices:
        rec_id = data_final.iloc[val[0]]['movieId']
        title = movies[movies['movieId'] == rec_id]['title'].values[0]
        rating = movie_rating_mean.get(rec_id, np.nan)  # Ambil rata-rata rating atau NaN jika tidak ada
        recommendations.append((title, rating, val[1]))

    return pd.DataFrame(recommendations, columns=['Title', 'Average Rating', 'Distance']).set_index('Distance')

"""# **7. Deep Learning Model (Optional - Embedding Based)**

Pada tahap ini, dilakukan pembangunan model rekomendasi berbasis *deep learning* yang memanfaatkan embedding untuk memetakan pengguna dan item (film) ke dalam vektor berdimensi rendah. Vektor ini mewakili preferensi pengguna dan karakteristik film dalam ruang laten.

### Tahapan model:

1. **Input Layer**: Model menerima pasangan input berupa ID pengguna dan ID film.
2. **Embedding Layer**: ID pengguna dan ID film diubah menjadi representasi vektor (embedding).
3. **Concatenation**: Embedding pengguna dan film digabungkan.
4. **Dense Layer**: Hasil gabungan diproses oleh layer fully connected (dense) untuk belajar representasi interaksi.
5. **Output Layer**: Menghasilkan prediksi rating (regresi).

### Tujuan:
Memprediksi rating yang mungkin diberikan pengguna terhadap suatu film berdasarkan pola interaksi sebelumnya.

Model dikompilasi dengan fungsi loss mean_squared_error, karena targetnya adalah memprediksi nilai rating kontinu.
"""

user_ids = ratings['userId'].unique().tolist()
movie_ids = ratings['movieId'].unique().tolist()

user2user_encoded = {x: i for i, x in enumerate(user_ids)}
movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}
movieencoded2movie = {i: x for i, x in enumerate(movie_ids)}

ratings['user'] = ratings['userId'].map(user2user_encoded)
ratings['movie'] = ratings['movieId'].map(movie2movie_encoded)

x = ratings[['user', 'movie']].values
y = ratings['rating'].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# 7. Deep Learning Model
# Jumlah user dan movie
num_users = len(user2user_encoded)
num_movies = len(movie2movie_encoded)

# Ukuran embedding
embedding_size = 50

# Input untuk user
user_input = Input(shape=(1,))
user_embedding = Embedding(num_users, embedding_size, embeddings_initializer='he_normal')(user_input)
user_vec = Flatten()(user_embedding)

# Input untuk movie
movie_input = Input(shape=(1,))
movie_embedding = Embedding(num_movies, embedding_size, embeddings_initializer='he_normal')(movie_input)
movie_vec = Flatten()(movie_embedding)

# Gabungkan user dan movie embedding
concat = Concatenate()([user_vec, movie_vec])
dense = Dense(128, activation='relu')(concat)
dropout = Dropout(0.5)(dense)
dense = Dense(64, activation='relu')(dropout)
output = Dense(1)(dense)

# Buat model
model = Model([user_input, movie_input], output)
model.compile(
    loss='mean_squared_error',
    optimizer=Adam(learning_rate=0.001),
    metrics=[MeanAbsoluteError()]
)

# Ringkasan model
model.summary()

# Training model
history = model.fit(
    [x_train[:, 0], x_train[:, 1]],
    y_train,
    validation_data=([x_test[:, 0], x_test[:, 1]], y_test),
    epochs=10,
    batch_size=64,
    verbose=1
)

# Plot hasil training
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss (MSE)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mean_absolute_error'], label='Train MAE')
plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')
plt.title('Mean Absolute Error (MAE)')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

plt.show()

"""# **8. Evaluasi dengan RMSE**"""

# Evaluasi dengan RMSE
test_preds = model.predict([x_test[:, 0], x_test[:, 1]])
rmse = np.sqrt(mean_squared_error(y_test, test_preds))
print(f"Test RMSE: {rmse:.4f}")

"""# **9. Fungsi Rekomendasi Berdasarkan User ID dengan Model DL**"""

def recommend_movies_for_user(user_id, num_recommendations=10):
    encoded_user_id = user2user_encoded.get(user_id)
    if encoded_user_id is None:
        return "User tidak ditemukan dalam dataset."

    movie_array = np.array([i for i in range(num_movies)])
    user_array = np.full(num_movies, encoded_user_id)

    predicted_ratings = model.predict([user_array, movie_array], verbose=0).flatten()
    top_indices = predicted_ratings.argsort()[-num_recommendations:][::-1]

    recommended_movie_ids = [movieencoded2movie[i] for i in top_indices]
    recommended_titles = movies[movies['movieId'].isin(recommended_movie_ids)][['movieId', 'title']]

    recommended_titles['Predicted Rating'] = predicted_ratings[top_indices]
    return recommended_titles.sort_values(by='Predicted Rating', ascending=False).reset_index(drop=True)

"""# **10. Contoh Pemakaian Fungsi**"""

get_movie_recommendation('Captain America: The Winter Soldier')

recommend_movies_for_user(user_id=3, num_recommendations=10)