# -*- coding: utf-8 -*-
"""Recomendation-Movie.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z62uhqSMcDCQO6n2hf_SFyJwh8Wi_aLH

# **1. Perkenalan Dataset**

## 📽️ Sistem Rekomendasi Film: Collaborative Filtering & Deep Learning

Proyek ini membangun sistem rekomendasi film menggunakan pendekatan:
- **Collaborative Filtering berbasis item (Item-to-Item KNN)**
- **Deep Learning berbasis embedding**

Dataset yang digunakan berasal dari **MovieLens Latest Small** dan terdiri dari:
- `movies.csv`: Informasi ID dan judul film.
- `ratings.csv`: Data rating dari pengguna terhadap film.

🎯 **Tujuan**: Memberikan rekomendasi film serupa berdasarkan pola penilaian pengguna lain serta mengeksplorasi model pembelajaran mendalam.

# **2. Import Library**

## 📦 Import Library

Library yang digunakan meliputi:
- `pandas`, `numpy`: Manipulasi dan analisis data.
- `sklearn`: Model KNN, metrik, dan pemrosesan.
- `tensorflow.keras`: Membangun model deep learning berbasis embedding.
- `matplotlib`, `seaborn`: Visualisasi data.
"""

import numpy as np
import pandas as pd
from sklearn.neighbors import NearestNeighbors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from scipy.sparse import csr_matrix
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import MeanAbsoluteError
from tensorflow.keras.regularizers import l2
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import mean_squared_error, mean_absolute_error

"""# **3. Memuat Dataset**

## 📥 Memuat Dataset

Dataset diimpor dari file CSV lokal. Berikut struktur dasar data:
- Jumlah film: {movies.shape[0]}
- Jumlah rating: {ratings.shape[0]}

Berikut beberapa baris awal dari masing-masing dataset:
"""

# Load dataset dari CSV
movies = pd.read_csv('data/movies.csv')
ratings = pd.read_csv('data/ratings.csv')

# Menampilkan beberapa baris awal
print("Movies dataset:")
movies.head()

print("Ratings dataset:")
ratings.head()

movies.shape

ratings.shape

"""# **4. Exploratory Data Analysis (EDA)**

## 🔍 Exploratory Data Analysis (EDA)

Tujuan EDA adalah memahami distribusi rating, aktivitas pengguna, dan popularitas film.

### 🎞️ Distribusi Jumlah Rating per Film
Menunjukkan berapa banyak pengguna yang memberi rating per film. Threshold minimum: 10 rating.

### 👤 Aktivitas Pengguna
Distribusi jumlah film yang dirating per pengguna. Threshold minimum: 60 rating.

### ⭐ Distribusi Rating
Distribusi rating pada skala 0-5. Sebagian besar rating berada di kisaran 3-5.

### 🏆 Top 10 Film Berdasarkan Rating Rata-rata (vote > 100)
Berikut film dengan penilaian tertinggi:
"""

# Bentuk pivot tabel dari rating
data = pd.pivot_table(ratings, index='movieId', columns='userId', values='rating')

# Handling Missing Values
data.fillna(0, inplace=True)

# Distribusi jumlah vote per movie dan per user
numberOf_user_voted_for_movie = ratings.groupby('movieId')['rating'].agg('count').reset_index()
numberOf_movies_voted_by_user = ratings.groupby('userId')['rating'].agg('count').reset_index()

# Visualisasi threshold filter
plt.figure()
sns.scatterplot(y='rating', x='movieId', data=numberOf_user_voted_for_movie)
plt.axhline(y=10, color='r')
plt.ylabel('Number Of Users Voted for Movie')
plt.title('Jumlah rating per movie')

plt.figure()
sns.scatterplot(y='rating', x='userId', data=numberOf_movies_voted_by_user)
plt.axhline(y=60, color='r')
plt.ylabel('Number Of Movies rated by user')
plt.title('Jumlah rating per user')
plt.show()

plt.figure()
sns.histplot(ratings['rating'], bins=10, kde=True)
plt.title("Distribusi Rating")
plt.show()

# Filter movies with at least 10 ratings and users with at least 60 ratings
top_movies = ratings.groupby('movieId')['rating'].agg(['mean', 'count'])
top_movies = top_movies.merge(movies, on='movieId')
top_movies.sort_values('count', ascending=False).head(10)

plt.figure(figsize=(8, 6))
sns.boxplot(x=ratings['rating'])
plt.title('Boxplot Distribusi Rating Film')
plt.xlabel('Rating')
plt.show()

# 2. Hitung jumlah rating per film dan rata-rata rating
movie_stats = ratings.groupby('movieId').agg(
    rating_count=('rating', 'count'),
    rating_mean=('rating', 'mean')
).reset_index()

# Filter film dengan minimal 100 rating
popular_movies = movie_stats[movie_stats['rating_count'] > 100]

# Gabungkan dengan data judul film
popular_movies = popular_movies.merge(movies[['movieId', 'title']], on='movieId')

# Ambil top 10 film berdasarkan rating rata-rata tertinggi
top10_movies = popular_movies.sort_values(by='rating_mean', ascending=False).head(10)

print("Top 10 Film dengan Rating Tertinggi (dengan minimal 100 rating):")
print(top10_movies[['title', 'rating_mean', 'rating_count']])

# Visualisasi top 10 film dengan barplot
plt.figure(figsize=(12, 6))
sns.barplot(data=top10_movies, x='rating_mean', y='title', palette='viridis')
plt.xlabel('Rata-rata Rating')
plt.ylabel('Judul Film')
plt.title('Top 10 Film dengan Rating Tertinggi (vote > 100)')
plt.xlim(0, 5)  # Rentang rating 0-5
plt.show()

"""# **5. Data Preprocessing**

## 🧹 Preprocessing Data

Data difilter untuk menghilangkan user dan film dengan interaksi terlalu sedikit:
- Hanya menyertakan film dengan >10 rating.
- Hanya menyertakan user yang memberi >60 rating.

Matrix rating diubah menjadi **Compressed Sparse Row (CSR)** untuk efisiensi pemrosesan.
"""

# 5. Data Preprocessing
data_final = data.loc[numberOf_user_voted_for_movie[numberOf_user_voted_for_movie['rating'] > 10]['movieId'], :]
data_final = data_final.loc[:, numberOf_movies_voted_by_user[numberOf_movies_voted_by_user['rating'] > 60]['userId']]
data_final.reset_index(inplace=True)

# Convert DataFrame to sparse matrix format
csr_data = csr_matrix(data_final.drop('movieId', axis=1).values)

"""# **6. Pembangunan Model Collaborative Filtering (KNN)**

Pada tahap ini, dilakukan pembangunan sistem rekomendasi menggunakan pendekatan **Collaborative Filtering berbasis item** dengan algoritma **K-Nearest Neighbors (KNN)**. Metode ini mencari kesamaan antar item (film) berdasarkan pola rating dari pengguna.

### Tahapan Model

1. **Representasi Data**  
   Dataset diubah menjadi **matriks sparse (CSR – Compressed Sparse Row)**, di mana baris merepresentasikan film dan kolom merepresentasikan pengguna, dengan nilai berupa rating.

2. **Pemodelan KNN**  
   Model KNN dilatih menggunakan **metrik cosine similarity** untuk mengukur kedekatan antar film berdasarkan pola rating pengguna.

3. **Fungsi Rekomendasi**
   - Menerima input berupa **nama film**.
   - Mencari **ID film** yang cocok.
   - Mengambil film-film yang paling mirip berdasarkan hasil dari `kneighbors`.
   - Mengembalikan daftar **rekomendasi film beserta tingkat kemiripannya** (cosine distance).

### Tujuan

Memberikan **rekomendasi film serupa** berdasarkan pola rating pengguna lain terhadap film tersebut. Pendekatan ini efektif ketika terdapat cukup data interaksi pengguna dan item, **meskipun tidak memperhitungkan informasi tambahan** seperti genre, sinopsis, atau profil pengguna.
"""

# 6. Pembangunan Model Collaborative Filtering (KNN)
# Buat TF-IDF matrix untuk judul film (sekali saja, di luar fungsi)
tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(movies['title'])

# Buat model KNN dengan metric cosine similarity dan algoritma brute force
knn = NearestNeighbors(metric='cosine', algorithm='brute')
knn.fit(csr_data)

# Precompute rata-rata rating tiap film untuk lookup cepat
movie_rating_mean = ratings.groupby('movieId')['rating'].mean()

def get_movie_recommendation(movie_name):
    n = 10

    # 1. Cari film dengan str.contains dulu
    movie_list = movies[movies['title'].str.contains(movie_name, case=False)]

    # 2. Jika tidak ada hasil, pakai TF-IDF + cosine similarity untuk cari judul mirip
    if len(movie_list) == 0:
        # Vectorize input movie_name
        movie_name_tfidf = tfidf.transform([movie_name])
        # Hitung cosine similarity dengan semua judul film
        cosine_sim = cosine_similarity(movie_name_tfidf, tfidf_matrix).flatten()
        # Ambil indeks film dengan similarity tertinggi > threshold (misal 0.3)
        max_sim_idx = np.argmax(cosine_sim)
        if cosine_sim[max_sim_idx] < 0.3:
            return "Tidak ditemukan judul film yang cocok. Silakan cek kembali input."
        movie_list = movies.iloc[[max_sim_idx]]

    movie_id = movie_list.iloc[0]['movieId']
    if movie_id not in data_final['movieId'].values:
        return "Film tersebut tidak ada dalam model rekomendasi."

    movie_idx = data_final[data_final['movieId'] == movie_id].index[0]
    distances, indices = knn.kneighbors(csr_data[movie_idx], n_neighbors=n+1)
    rec_movie_indices = sorted(list(zip(indices.squeeze(), distances.squeeze())), key=lambda x: x[1])[1:]

    recommendations = []
    for val in rec_movie_indices:
        rec_id = data_final.iloc[val[0]]['movieId']
        title = movies[movies['movieId'] == rec_id]['title'].values[0]
        rating = movie_rating_mean.get(rec_id, np.nan)  # Ambil rata-rata rating atau NaN jika tidak ada
        recommendations.append((title, rating, val[1]))

    return pd.DataFrame(recommendations, columns=['Title', 'Average Rating', 'Distance']).set_index('Distance')

# Quantitative Evaluation for KNN Model
def evaluate_knn_model(user_id, n_recommendations=10):
    """
    Evaluate KNN model for a specific user using precision@k and recall@k

    Parameters:
    - user_id: ID of user to evaluate
    - n_recommendations: Number of recommendations to consider (k)
    """
    # Get actual high-rated movies by user
    actual_high_rated = ratings[
        (ratings['userId'] == user_id) &
        (ratings['rating'] >= 4.0)
    ]['movieId'].tolist()

    # Skip if user doesn't have enough high-rated movies
    if len(actual_high_rated) < 5:
        return None, None, None

    # Get recommendations using collaborative filtering
    recommended_movies = []
    for movie_id in actual_high_rated[:5]:  # Use first 5 high-rated movies as input
        movie_title = movies[movies['movieId'] == movie_id]['title'].values[0]
        recommendations = get_movie_recommendation(movie_title)
        recommended_movies.extend(recommendations['Title'].tolist())

    # Remove duplicates
    recommended_movies = list(set(recommended_movies))[:n_recommendations]

    # Convert titles to movie IDs
    recommended_ids = []
    for title in recommended_movies:
        match = movies[movies['title'] == title]
        if not match.empty:
            recommended_ids.append(match['movieId'].values[0])

    # Calculate precision and recall
    relevant_recommended = len(set(recommended_ids) & set(actual_high_rated))
    precision = relevant_recommended / n_recommendations
    recall = relevant_recommended / len(actual_high_rated)

    return precision, recall, len(actual_high_rated)

# Evaluate on sample users
sample_users = [1, 15, 30, 45, 60]
precisions = []
recalls = []

for user_id in sample_users:
    precision, recall, n_high_rated = evaluate_knn_model(user_id)
    if precision is not None:
        precisions.append(precision)
        recalls.append(recall)
        print(f"User {user_id} (with {n_high_rated} high-rated movies):")
        print(f"  Precision@10: {precision:.2f}, Recall@10: {recall:.2f}")

if precisions:
    print(f"\nAverage Precision@10: {np.mean(precisions):.2f}")
    print(f"Average Recall@10: {np.mean(recalls):.2f}")

"""# **7. Deep Learning Model (Optional - Embedding Based)**

Pada tahap ini, dilakukan pembangunan model rekomendasi berbasis *deep learning* yang memanfaatkan embedding untuk memetakan pengguna dan item (film) ke dalam vektor berdimensi rendah. Vektor ini mewakili preferensi pengguna dan karakteristik film dalam ruang laten.

### Tahapan model:

1. **Input Layer**: Model menerima pasangan input berupa ID pengguna dan ID film.
2. **Embedding Layer**: ID pengguna dan ID film diubah menjadi representasi vektor (embedding).
3. **Concatenation**: Embedding pengguna dan film digabungkan.
4. **Dense Layer**: Hasil gabungan diproses oleh layer fully connected (dense) untuk belajar representasi interaksi.
5. **Output Layer**: Menghasilkan prediksi rating (regresi).

### Tujuan:
Memprediksi rating yang mungkin diberikan pengguna terhadap suatu film berdasarkan pola interaksi sebelumnya.

Model dikompilasi dengan fungsi loss mean_squared_error, karena targetnya adalah memprediksi nilai rating kontinu.
"""

print("\nEncoding user and movie IDs for Deep Learning model...")
user_ids = ratings['userId'].unique().tolist()
movie_ids = ratings['movieId'].unique().tolist()

user2user_encoded = {x: i for i, x in enumerate(user_ids)}
movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}
movieencoded2movie = {i: x for i, x in enumerate(movie_ids)}

ratings['user'] = ratings['userId'].map(user2user_encoded)
ratings['movie'] = ratings['movieId'].map(movie2movie_encoded)

print(f"Total users: {len(user2user_encoded)}")
print(f"Total movies: {len(movie2movie_encoded)}")

print("\nSplitting data into training and test sets...")
x = ratings[['user', 'movie']].values
y = ratings['rating'].values

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

print(f"Training data shape: {x_train.shape}")
print(f"Testing data shape: {x_test.shape}")

# 7. Deep Learning Model
# Jumlah user dan movie
num_users = len(user2user_encoded)
num_movies = len(movie2movie_encoded)

# Ukuran embedding
embedding_size = 50

# Input untuk user
user_input = Input(shape=(1,))
user_embedding = Embedding(num_users, embedding_size, embeddings_initializer='he_normal')(user_input)
user_vec = Flatten()(user_embedding)

# Input untuk movie
movie_input = Input(shape=(1,))
movie_embedding = Embedding(num_movies, embedding_size, embeddings_initializer='he_normal')(movie_input)
movie_vec = Flatten()(movie_embedding)

# Gabungkan user dan movie embedding
concat = Concatenate()([user_vec, movie_vec])
dense = Dense(128, activation='relu', kernel_regularizer=l2(1e-5))(concat)
batchnorm1 = BatchNormalization()(dense)
dropout1 = Dropout(0.6)(batchnorm1)

dense2 = Dense(64, activation='relu', kernel_regularizer=l2(1e-5))(dropout1)
batchnorm2 = BatchNormalization()(dense2)
dropout2 = Dropout(0.5)(batchnorm2)

output = Dense(1)(dropout2)

# Buat model
model = Model([user_input, movie_input], output)
model.compile(
    loss=MeanSquaredError(),
    optimizer=Adam(learning_rate=0.001),
    metrics=[MeanAbsoluteError()]
)

# Ringkasan model
model.summary()

# Callbacks
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=1e-6,
    verbose=1
)

# Training model
history = model.fit(
    [x_train[:, 0], x_train[:, 1]],
    y_train,
    validation_data=([x_test[:, 0], x_test[:, 1]], y_test),
    epochs=50,
    batch_size=128,
    verbose=1,
    callbacks=[early_stop, reduce_lr]
)

# Plot hasil training
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss (MSE)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mean_absolute_error'], label='Train MAE')
plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')
plt.title('Mean Absolute Error (MAE)')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

plt.show()

"""# **8. Evaluasi dengan RMSE**"""

# Prediksi di data test
y_pred = model.predict([x_test[:, 0], x_test[:, 1]]).flatten()
y_true = y_test

# Evaluasi metrik
rmse = np.sqrt(mean_squared_error(y_true, y_pred))
mae = mean_absolute_error(y_true, y_pred)
mse = mean_squared_error(y_true, y_pred)

# Evaluasi dari model.evaluate (menggunakan loss & MAE internal model)
test_loss, test_mae = model.evaluate([x_test[:, 0], x_test[:, 1]], y_test, verbose=0)

# Tampilkan hasil
print("=== Evaluasi Model ===")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")

# Visualisasi y_test vs y_pred
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.3)
plt.xlabel("True Ratings")
plt.ylabel("Predicted Ratings")
plt.title("Visualisasi y_test vs y_pred")
plt.grid(True)
plt.show()

# Consistent Deep Learning Evaluation
# Re-evaluate to ensure consistency
test_loss, test_mae = model.evaluate([x_test[:, 0], x_test[:, 1]], y_test, verbose=0)
y_pred = model.predict([x_test[:, 0], x_test[:, 1]]).flatten()
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print("\n=== Deep Learning Model Evaluation ===")
print(f"Test Loss (MSE): {test_loss:.4f}")
print(f"Test MAE: {test_mae:.4f}")
print(f"Calculated MSE: {mse:.4f}")
print(f"Calculated RMSE: {rmse:.4f}")

# Save metrics for reporting
dl_metrics = {
    'MAE': round(test_mae, 4),
    'MSE': round(mse, 4),
    'RMSE': round(rmse, 4)
}

"""# **9. Fungsi Rekomendasi Berdasarkan User ID dengan Model DL**"""

def recommend_movies_for_user(user_id, num_recommendations=10):
    encoded_user_id = user2user_encoded.get(user_id)
    if encoded_user_id is None:
        return "User tidak ditemukan dalam dataset."

    movie_array = np.array([i for i in range(num_movies)])
    user_array = np.full(num_movies, encoded_user_id)

    predicted_ratings = model.predict([user_array, movie_array], verbose=0).flatten()
    top_indices = predicted_ratings.argsort()[-num_recommendations:][::-1]

    recommended_movie_ids = [movieencoded2movie[i] for i in top_indices]
    recommended_titles = movies[movies['movieId'].isin(recommended_movie_ids)][['movieId', 'title']]

    recommended_titles['Predicted Rating'] = predicted_ratings[top_indices]
    return recommended_titles.sort_values(by='Predicted Rating', ascending=False).reset_index(drop=True)

"""# **10. Contoh Pemakaian Fungsi**"""

get_movie_recommendation('Captain America: The Winter Soldier')

print("Deep Learning Recommendations for User ID 3:")
dl_recommendations = recommend_movies_for_user(user_id=3, num_recommendations=10)
display(dl_recommendations)

# Visualize the recommendations
plt.figure(figsize=(10, 6))
sns.barplot(data=dl_recommendations, x='Predicted Rating', y='title', palette='viridis')
plt.title('Top 10 Recommendations for User 3 (Deep Learning Model)')
plt.xlabel('Predicted Rating')
plt.ylabel('Movie Title')
plt.tight_layout()
plt.show()